{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 11:25:29 [INFO] __main__: SharedMemory initialized with database: multiagent_memory.db\n",
      "2025-05-30 11:25:29 [INFO] __main__: Processing input for conversation 6dbe6ee95a8a\n",
      "2025-05-30 11:25:29 [INFO] httpx: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-30 11:25:29 [INFO] __main__: Classification: ClassificationResult(format=<InputFormat.EMAIL: 'Email'>, intent=<IntentType.COMPLAINT: 'Complaint'>, confidence=0.95, reasoning='The content contains a clear subject line indicating a product complaint, and the body of the email describes a specific issue with a purchased product, which strongly suggests a complaint. The tone is also urgent, which is consistent with a complaint intent.')\n",
      "2025-05-30 11:25:29 [INFO] __main__: Classified as Email with intent Complaint (confidence: 0.95)\n",
      "2025-05-30 11:25:30 [INFO] httpx: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-30 11:25:30 [INFO] __main__: Processing completed for 6dbe6ee95a8a in 1.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conversation_id\": \"6dbe6ee95a8a\",\n",
      "  \"classification\": {\n",
      "    \"format\": \"Email\",\n",
      "    \"intent\": \"Complaint\",\n",
      "    \"confidence\": 0.95,\n",
      "    \"reasoning\": \"The content contains a clear subject line indicating a product complaint, and the body of the email describes a specific issue with a purchased product, which strongly suggests a complaint. The tone is also urgent, which is consistent with a complaint intent.\"\n",
      "  },\n",
      "  \"processing_result\": {\n",
      "    \"success\": true,\n",
      "    \"data\": {\n",
      "      \"sender\": {\n",
      "        \"email\": \"customer@example.com\",\n",
      "        \"name\": \"John Doe\"\n",
      "      },\n",
      "      \"urgency\": \"High\",\n",
      "      \"topics\": [\n",
      "        \"Product Complaint\",\n",
      "        \"Laptop Issue\"\n",
      "      ],\n",
      "      \"action_items\": [\n",
      "        \"Resolve the issue urgently\"\n",
      "      ],\n",
      "      \"summary\": \"Customer is complaining about a laptop screen flickering issue within 3 days of purchase and requests urgent resolution.\",\n",
      "      \"entities\": {\n",
      "        \"person\": \"John Doe\",\n",
      "        \"organization\": null,\n",
      "        \"location\": null\n",
      "      },\n",
      "      \"intent_specific_data\": {\n",
      "        \"order_id\": \"45321\",\n",
      "        \"product\": \"laptop\"\n",
      "      },\n",
      "      \"confidence_score\": 0.95\n",
      "    },\n",
      "    \"anomalies\": [],\n",
      "    \"processing_time\": 0.5751430988311768,\n",
      "    \"agent_used\": \"Email_Agent\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"conversation_summary\": {\n",
      "      \"total_interactions\": 1,\n",
      "      \"formats_processed\": [\n",
      "        \"Email\"\n",
      "      ],\n",
      "      \"intents_detected\": [\n",
      "        \"Complaint\"\n",
      "      ],\n",
      "      \"agents_used\": [\n",
      "        \"Email_Agent\"\n",
      "      ],\n",
      "      \"latest_timestamp\": \"2025-05-30T05:55:30.340638+00:00\",\n",
      "      \"latest_extraction\": {\n",
      "        \"sender\": {\n",
      "          \"email\": \"customer@example.com\",\n",
      "          \"name\": \"John Doe\"\n",
      "        },\n",
      "        \"urgency\": \"High\",\n",
      "        \"topics\": [\n",
      "          \"Product Complaint\",\n",
      "          \"Laptop Issue\"\n",
      "        ],\n",
      "        \"action_items\": [\n",
      "          \"Resolve the issue urgently\"\n",
      "        ],\n",
      "        \"summary\": \"Customer is complaining about a laptop screen flickering issue within 3 days of purchase and requests urgent resolution.\",\n",
      "        \"entities\": {\n",
      "          \"person\": \"John Doe\",\n",
      "          \"organization\": null,\n",
      "          \"location\": null\n",
      "        },\n",
      "        \"intent_specific_data\": {\n",
      "          \"order_id\": \"45321\",\n",
      "          \"product\": \"laptop\"\n",
      "        },\n",
      "        \"confidence_score\": 0.95\n",
      "      }\n",
      "    },\n",
      "    \"total_interactions\": 1\n",
      "  },\n",
      "  \"performance\": {\n",
      "    \"total_processing_time\": 1.215247631072998,\n",
      "    \"agent_processing_time\": 0.5751430988311768\n",
      "  },\n",
      "  \"metadata\": {},\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import logging\n",
    "import sqlite3\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, Union, Optional, List\n",
    "import re\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import traceback\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "# ----------- CONFIGURATION ------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler('multiagent_system.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Environment validation\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    logger.error(\"GROQ_API_KEY not set in environment variables.\")\n",
    "    raise ValueError(\"GROQ_API_KEY is required\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# ----------- ENUMS AND DATA STRUCTURES ------------\n",
    "class InputFormat(Enum):\n",
    "    PDF = \"PDF\"\n",
    "    JSON = \"JSON\"\n",
    "    EMAIL = \"Email\"\n",
    "    UNKNOWN = \"Unknown\"\n",
    "\n",
    "class IntentType(Enum):\n",
    "    INVOICE = \"Invoice\"\n",
    "    RFQ = \"RFQ\"\n",
    "    COMPLAINT = \"Complaint\"\n",
    "    REGULATION = \"Regulation\"\n",
    "    OTHER = \"Other\"\n",
    "\n",
    "@dataclass\n",
    "class ClassificationResult:\n",
    "    format: InputFormat\n",
    "    intent: IntentType\n",
    "    confidence: float = 0.0\n",
    "    reasoning: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    success: bool\n",
    "    data: Dict[str, Any]\n",
    "    anomalies: List[str]\n",
    "    processing_time: float\n",
    "    agent_used: str\n",
    "\n",
    "@dataclass\n",
    "class MemoryEntry:\n",
    "    conversation_id: str\n",
    "    timestamp: str\n",
    "    source: str\n",
    "    data_type: str\n",
    "    intent: str\n",
    "    extracted_values: Dict[str, Any]\n",
    "    agent_used: str\n",
    "    processing_result: Dict[str, Any]\n",
    "\n",
    "# ----------- ENHANCED SHARED MEMORY ------------\n",
    "class SharedMemory:\n",
    "    def __init__(self, db_path: str = \"multiagent_memory.db\"):\n",
    "        self.db_path = db_path\n",
    "        self._init_database()\n",
    "        logger.info(f\"SharedMemory initialized with database: {db_path}\")\n",
    "\n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize SQLite database for persistent memory\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                conn.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS memory_entries (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        conversation_id TEXT NOT NULL,\n",
    "                        timestamp TEXT NOT NULL,\n",
    "                        source TEXT NOT NULL,\n",
    "                        data_type TEXT NOT NULL,\n",
    "                        intent TEXT NOT NULL,\n",
    "                        extracted_values TEXT NOT NULL,\n",
    "                        agent_used TEXT NOT NULL,\n",
    "                        processing_result TEXT NOT NULL,\n",
    "                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "                    )\n",
    "                \"\"\")\n",
    "                conn.execute(\"\"\"\n",
    "                    CREATE INDEX IF NOT EXISTS idx_conversation_id \n",
    "                    ON memory_entries(conversation_id)\n",
    "                \"\"\")\n",
    "                conn.execute(\"\"\"\n",
    "                    CREATE INDEX IF NOT EXISTS idx_timestamp \n",
    "                    ON memory_entries(timestamp)\n",
    "                \"\"\")\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database initialization failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def log(self, entry: MemoryEntry) -> bool:\n",
    "        \"\"\"Log entry to persistent storage\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                conn.execute(\"\"\"\n",
    "                    INSERT INTO memory_entries \n",
    "                    (conversation_id, timestamp, source, data_type, intent, \n",
    "                     extracted_values, agent_used, processing_result)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    entry.conversation_id,\n",
    "                    entry.timestamp,\n",
    "                    entry.source,\n",
    "                    entry.data_type,\n",
    "                    entry.intent,\n",
    "                    json.dumps(entry.extracted_values),\n",
    "                    entry.agent_used,\n",
    "                    json.dumps(entry.processing_result)\n",
    "                ))\n",
    "                conn.commit()\n",
    "            logger.debug(f\"Logged entry for conversation {entry.conversation_id}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to log memory entry: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_context(self, conversation_id: str, limit: int = 10) -> List[MemoryEntry]:\n",
    "        \"\"\"Retrieve conversation context\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.execute(\"\"\"\n",
    "                    SELECT conversation_id, timestamp, source, data_type, intent,\n",
    "                           extracted_values, agent_used, processing_result\n",
    "                    FROM memory_entries\n",
    "                    WHERE conversation_id = ?\n",
    "                    ORDER BY timestamp DESC\n",
    "                    LIMIT ?\n",
    "                \"\"\", (conversation_id, limit))\n",
    "                \n",
    "                entries = []\n",
    "                for row in cursor.fetchall():\n",
    "                    entries.append(MemoryEntry(\n",
    "                        conversation_id=row[0],\n",
    "                        timestamp=row[1],\n",
    "                        source=row[2],\n",
    "                        data_type=row[3],\n",
    "                        intent=row[4],\n",
    "                        extracted_values=json.loads(row[5]),\n",
    "                        agent_used=row[6],\n",
    "                        processing_result=json.loads(row[7])\n",
    "                    ))\n",
    "                return entries\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to retrieve context: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_last_extraction(self, conversation_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get the most recent extraction for context\"\"\"\n",
    "        entries = self.get_context(conversation_id, limit=1)\n",
    "        return entries[0].extracted_values if entries else {}\n",
    "\n",
    "    def get_conversation_summary(self, conversation_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get summarized conversation context\"\"\"\n",
    "        entries = self.get_context(conversation_id)\n",
    "        if not entries:\n",
    "            return {}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_interactions\": len(entries),\n",
    "            \"formats_processed\": list(set(entry.data_type for entry in entries)),\n",
    "            \"intents_detected\": list(set(entry.intent for entry in entries)),\n",
    "            \"agents_used\": list(set(entry.agent_used for entry in entries)),\n",
    "            \"latest_timestamp\": entries[0].timestamp if entries else None,\n",
    "            \"latest_extraction\": entries[0].extracted_values if entries else {}\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "shared_memory = SharedMemory()\n",
    "\n",
    "# ----------- ENHANCED GROQ COMPLETION UTILS ------------\n",
    "def groq_completion(\n",
    "    prompt: str,\n",
    "    system: str = \"\",\n",
    "    model: str = \"llama3-70b-8192\",\n",
    "    response_format: str = None,\n",
    "    max_retries: int = 3,\n",
    "    timeout: int = 30,\n",
    "    temperature: float = 0.3\n",
    ") -> Any:\n",
    "    \"\"\"Enhanced Groq completion with better error handling and validation\"\"\"\n",
    "    \n",
    "    if not prompt.strip():\n",
    "        raise ValueError(\"Prompt cannot be empty\")\n",
    "    \n",
    "    if response_format == \"json\" and \"json\" not in prompt.lower():\n",
    "        prompt += \"\\n\\nPlease respond in valid JSON format only.\"\n",
    "\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    last_error = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logger.debug(f\"Groq call attempt {attempt}/{max_retries}\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"} if response_format == \"json\" else None,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            if not content:\n",
    "                raise ValueError(\"Empty response from Groq\")\n",
    "                \n",
    "            # Validate JSON if requested\n",
    "            if response_format == \"json\":\n",
    "                try:\n",
    "                    json.loads(content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    raise ValueError(f\"Invalid JSON response: {e}\")\n",
    "                    \n",
    "            return content\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            logger.warning(f\"Groq completion failed (attempt {attempt}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(2 ** attempt)\n",
    "    \n",
    "    logger.error(f\"Groq completion failed after {max_retries} retries: {last_error}\")\n",
    "    raise RuntimeError(f\"Groq completion failed: {last_error}\")\n",
    "\n",
    "# ----------- ENHANCED CLASSIFIER AGENT ------------\n",
    "def classify_agent(input_data: Union[str, bytes]) -> ClassificationResult:\n",
    "    \"\"\"Enhanced classification with confidence scoring and reasoning\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # PDF detection\n",
    "        if isinstance(input_data, bytes) and input_data.startswith(b'%PDF'):\n",
    "            return ClassificationResult(\n",
    "                format=InputFormat.PDF,\n",
    "                intent=IntentType.OTHER,  # Will be determined after text extraction\n",
    "                confidence=1.0,\n",
    "                reasoning=\"PDF magic number detected\"\n",
    "            )\n",
    "        \n",
    "        # Convert bytes to string if needed\n",
    "        if isinstance(input_data, bytes):\n",
    "            try:\n",
    "                content = input_data.decode('utf-8', errors='ignore')[:3000]\n",
    "            except Exception:\n",
    "                content = str(input_data)[:3000]\n",
    "        else:\n",
    "            content = str(input_data)[:3000]\n",
    "\n",
    "        # Enhanced JSON detection\n",
    "        is_json = False\n",
    "        try:\n",
    "            json.loads(content)\n",
    "            is_json = True\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            pass\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert document classifier. Analyze the input and classify it with high accuracy.\n",
    "        \n",
    "        Format Classification:\n",
    "        - PDF: Documents with PDF structure\n",
    "        - JSON: Valid JSON data structures\n",
    "        - Email: Email content, messages, or communication text\n",
    "        \n",
    "        Intent Classification:\n",
    "        - Invoice: Billing documents, payment requests, financial statements\n",
    "        - RFQ: Request for Quote, procurement requests, vendor inquiries\n",
    "        - Complaint: Customer complaints, issues, grievances\n",
    "        - Regulation: Legal documents, compliance materials, policy documents\n",
    "        - Other: General content that doesn't fit above categories\n",
    "        \n",
    "        Provide confidence score (0.0-1.0) and reasoning for your classification.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Content to classify:\n",
    "        {content}\n",
    "        \n",
    "        Analysis requirements:\n",
    "        1. Determine format (PDF/JSON/Email)\n",
    "        2. Determine intent (Invoice/RFQ/Complaint/Regulation/Other)\n",
    "        3. Provide confidence score (0.0-1.0)\n",
    "        4. Provide reasoning for classification\n",
    "        \n",
    "        Output JSON with keys: 'format', 'intent', 'confidence', 'reasoning'\n",
    "        \"\"\"\n",
    "\n",
    "        raw = groq_completion(prompt, system_prompt, response_format=\"json\")\n",
    "        result = json.loads(raw)\n",
    "        \n",
    "        # Override format if JSON was detected programmatically\n",
    "        if is_json:\n",
    "            result['format'] = 'JSON'\n",
    "            result['confidence'] = max(result.get('confidence', 0.5), 0.9)\n",
    "        \n",
    "        classification = ClassificationResult(\n",
    "            format=InputFormat(result.get('format', 'Email')),\n",
    "            intent=IntentType(result.get('intent', 'Other')),\n",
    "            confidence=float(result.get('confidence', 0.5)),\n",
    "            reasoning=result.get('reasoning', 'No reasoning provided')\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Classification: {classification}\")\n",
    "        return classification\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Classification error: {e}\")\n",
    "        return ClassificationResult(\n",
    "            format=InputFormat.EMAIL,\n",
    "            intent=IntentType.OTHER,\n",
    "            confidence=0.1,\n",
    "            reasoning=f\"Classification failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ----------- ENHANCED JSON AGENT ------------\n",
    "def json_agent(payload: dict, conversation_id: str, intent: IntentType) -> ProcessingResult:\n",
    "    \"\"\"Enhanced JSON processing with schema validation and anomaly detection\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get conversation context\n",
    "        context_summary = shared_memory.get_conversation_summary(conversation_id)\n",
    "        \n",
    "        # Define target schemas based on intent\n",
    "        schema_definitions = {\n",
    "            IntentType.INVOICE: {\n",
    "                \"required_fields\": [\"amount\", \"date\", \"vendor\", \"invoice_number\"],\n",
    "                \"optional_fields\": [\"tax_amount\", \"due_date\", \"description\", \"line_items\"]\n",
    "            },\n",
    "            IntentType.RFQ: {\n",
    "                \"required_fields\": [\"items\", \"quantity\", \"deadline\", \"requester\"],\n",
    "                \"optional_fields\": [\"specifications\", \"budget\", \"delivery_location\"]\n",
    "            },\n",
    "            IntentType.COMPLAINT: {\n",
    "                \"required_fields\": [\"issue\", \"customer\", \"date\"],\n",
    "                \"optional_fields\": [\"severity\", \"resolution_requested\", \"previous_interactions\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        target_schema = schema_definitions.get(intent, {\n",
    "            \"required_fields\": [\"type\", \"content\"],\n",
    "            \"optional_fields\": [\"metadata\", \"source\"]\n",
    "        })\n",
    "        \n",
    "        system_prompt = f\"\"\"\n",
    "        You are a JSON processing expert specializing in {intent.value} documents.\n",
    "        Extract and validate data according to the target schema.\n",
    "        \n",
    "        Target Schema:\n",
    "        - Required fields: {target_schema['required_fields']}\n",
    "        - Optional fields: {target_schema['optional_fields']}\n",
    "        \n",
    "        Flag any anomalies such as:\n",
    "        - Missing required fields\n",
    "        - Invalid data types\n",
    "        - Suspicious values\n",
    "        - Inconsistent data\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Conversation Context Summary:\n",
    "        {json.dumps(context_summary, indent=2)}\n",
    "        \n",
    "        Input JSON Payload:\n",
    "        {json.dumps(payload, indent=2)}\n",
    "        \n",
    "        Tasks:\n",
    "        1. Extract data matching the target schema\n",
    "        2. Validate all fields\n",
    "        3. Flag any anomalies or errors\n",
    "        4. Provide data quality score (0.0-1.0)\n",
    "        \n",
    "        Output JSON with keys: 'extracted_data', 'anomalies', 'data_quality_score'\n",
    "        \"\"\"\n",
    "\n",
    "        raw = groq_completion(prompt, system_prompt, response_format=\"json\")\n",
    "        result = json.loads(raw)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return ProcessingResult(\n",
    "            success=True,\n",
    "            data=result.get('extracted_data', {}),\n",
    "            anomalies=result.get('anomalies', []),\n",
    "            processing_time=processing_time,\n",
    "            agent_used=\"JSON_Agent\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.error(f\"JSON agent error: {e}\")\n",
    "        return ProcessingResult(\n",
    "            success=False,\n",
    "            data={},\n",
    "            anomalies=[f\"Processing failed: {str(e)}\"],\n",
    "            processing_time=processing_time,\n",
    "            agent_used=\"JSON_Agent\"\n",
    "        )\n",
    "\n",
    "# ----------- ENHANCED EMAIL AGENT ------------\n",
    "def email_agent(content: str, conversation_id: str, intent: IntentType) -> ProcessingResult:\n",
    "    \"\"\"Enhanced email processing with intent-specific extraction\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Clean and preprocess content\n",
    "        cleaned_content = re.sub(r'\\s+', ' ', content.strip())\n",
    "        if len(cleaned_content) > 8000:\n",
    "            cleaned_content = cleaned_content[:8000] + \"... [truncated]\"\n",
    "        \n",
    "        # Get conversation context\n",
    "        context_summary = shared_memory.get_conversation_summary(conversation_id)\n",
    "        \n",
    "        # Intent-specific extraction prompts\n",
    "        intent_prompts = {\n",
    "            IntentType.INVOICE: \"Focus on extracting billing information, amounts, dates, and vendor details.\",\n",
    "            IntentType.RFQ: \"Focus on extracting requested items, quantities, deadlines, and specifications.\",\n",
    "            IntentType.COMPLAINT: \"Focus on extracting the issue, customer information, and resolution requests.\",\n",
    "            IntentType.REGULATION: \"Focus on extracting regulatory requirements, compliance details, and deadlines.\"\n",
    "        }\n",
    "        \n",
    "        specific_prompt = intent_prompts.get(intent, \"Extract key information relevant to this communication.\")\n",
    "        \n",
    "        system_prompt = f\"\"\"\n",
    "        You are an email processing expert specializing in {intent.value} communications.\n",
    "        Extract comprehensive information with high accuracy.\n",
    "        \n",
    "        {specific_prompt}\n",
    "        \n",
    "        Always extract:\n",
    "        - Sender information\n",
    "        - Urgency level (Low/Medium/High/Critical)\n",
    "        - Key topics and entities\n",
    "        - Action items or requests\n",
    "        - Summary of content\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Conversation Context Summary:\n",
    "        {json.dumps(context_summary, indent=2) if context_summary else \"No previous context\"}\n",
    "        \n",
    "        Email Content:\n",
    "        {cleaned_content}\n",
    "        \n",
    "        Extract and structure the following information:\n",
    "        1. sender: Email sender details\n",
    "        2. urgency: Urgency level (Low/Medium/High/Critical)\n",
    "        3. topics: List of main topics discussed\n",
    "        4. action_items: List of requested actions\n",
    "        5. summary: Comprehensive summary\n",
    "        6. entities: Named entities (people, organizations, locations)\n",
    "        7. intent_specific_data: Data specific to {intent.value}\n",
    "        8. confidence_score: Extraction confidence (0.0-1.0)\n",
    "        \n",
    "        Output JSON with these exact keys.\n",
    "        \"\"\"\n",
    "\n",
    "        raw = groq_completion(\n",
    "            prompt, \n",
    "            system_prompt, \n",
    "            model=\"llama3-70b-8192\",\n",
    "            response_format=\"json\"\n",
    "        )\n",
    "        \n",
    "        result = json.loads(raw)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Validate extracted data\n",
    "        anomalies = []\n",
    "        if not result.get('sender'):\n",
    "            anomalies.append(\"No sender information found\")\n",
    "        if not result.get('summary'):\n",
    "            anomalies.append(\"No summary generated\")\n",
    "        if result.get('confidence_score', 0) < 0.5:\n",
    "            anomalies.append(\"Low extraction confidence\")\n",
    "        \n",
    "        return ProcessingResult(\n",
    "            success=True,\n",
    "            data=result,\n",
    "            anomalies=anomalies,\n",
    "            processing_time=processing_time,\n",
    "            agent_used=\"Email_Agent\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.error(f\"Email agent error: {e}\")\n",
    "        return ProcessingResult(\n",
    "            success=False,\n",
    "            data={\"error\": str(e)},\n",
    "            anomalies=[f\"Processing failed: {str(e)}\"],\n",
    "            processing_time=processing_time,\n",
    "            agent_used=\"Email_Agent\"\n",
    "        )\n",
    "\n",
    "# ----------- ENHANCED PDF AGENT ------------\n",
    "def pdf_agent(pdf_bytes: bytes, conversation_id: str) -> ProcessingResult:\n",
    "    \"\"\"Enhanced PDF processing with better text extraction and error handling\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Try multiple PDF libraries for robustness\n",
    "        text = \"\"\n",
    "        extraction_method = \"unknown\"\n",
    "        \n",
    "        # Method 1: PyPDF2\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader\n",
    "            from io import BytesIO\n",
    "            \n",
    "            reader = PdfReader(BytesIO(pdf_bytes))\n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += f\"[Page {page_num + 1}]\\n{page_text}\\n\"\n",
    "                if len(text) > 15000:  # Reasonable limit\n",
    "                    break\n",
    "            extraction_method = \"PyPDF2\"\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.warning(\"PyPDF2 not available, trying alternative methods\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"PyPDF2 extraction failed: {e}\")\n",
    "        \n",
    "        # Method 2: pdfplumber (if available)\n",
    "        if not text:\n",
    "            try:\n",
    "                import pdfplumber\n",
    "                from io import BytesIO\n",
    "                \n",
    "                with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:\n",
    "                    for page_num, page in enumerate(pdf.pages):\n",
    "                        page_text = page.extract_text() or \"\"\n",
    "                        text += f\"[Page {page_num + 1}]\\n{page_text}\\n\"\n",
    "                        if len(text) > 15000:\n",
    "                            break\n",
    "                extraction_method = \"pdfplumber\"\n",
    "                \n",
    "            except ImportError:\n",
    "                logger.warning(\"pdfplumber not available\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"pdfplumber extraction failed: {e}\")\n",
    "        \n",
    "        if not text:\n",
    "            raise ValueError(\"No text could be extracted from PDF\")\n",
    "        \n",
    "        logger.info(f\"Extracted {len(text)} characters from PDF using {extraction_method}\")\n",
    "        \n",
    "        # Re-classify the extracted text to determine intent\n",
    "        classification = classify_agent(text[:2000])\n",
    "        intent = classification.intent\n",
    "        \n",
    "        # Process extracted text as email content\n",
    "        email_result = email_agent(text, conversation_id, intent)\n",
    "        \n",
    "        # Enhance with PDF-specific metadata\n",
    "        pdf_metadata = {\n",
    "            \"extraction_method\": extraction_method,\n",
    "            \"text_length\": len(text),\n",
    "            \"estimated_pages\": text.count(\"[Page\"),\n",
    "            \"classification_confidence\": classification.confidence\n",
    "        }\n",
    "        \n",
    "        if email_result.success:\n",
    "            email_result.data[\"pdf_metadata\"] = pdf_metadata\n",
    "        \n",
    "        email_result.agent_used = \"PDF_Agent\"\n",
    "        email_result.processing_time = time.time() - start_time\n",
    "        \n",
    "        return email_result\n",
    "        \n",
    "    except ImportError as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.error(\"PDF processing libraries not available\")\n",
    "        return ProcessingResult(\n",
    "            success=False,\n",
    "            data={\"error\": \"PDF processing libraries not installed\"},\n",
    "            anomalies=[\"PyPDF2 or pdfplumber required for PDF processing\"],\n",
    "            processing_time=processing_time,\n",
    "            agent_used=\"PDF_Agent\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.exception(\"PDF processing failed\")\n",
    "        return ProcessingResult(\n",
    "            success=False,\n",
    "            data={\"error\": str(e)},\n",
    "            anomalies=[f\"PDF processing failed: {str(e)}\"],\n",
    "            processing_time=processing_time,\n",
    "            agent_used=\"PDF_Agent\"\n",
    "        )\n",
    "\n",
    "# ----------- ENHANCED MAIN ORCHESTRATOR ------------\n",
    "def process_input(\n",
    "    input_data: Any, \n",
    "    conversation_id: Optional[str] = None,\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced main orchestrator with comprehensive error handling and logging\n",
    "    \"\"\"\n",
    "    \n",
    "    if not conversation_id:\n",
    "        conversation_id = hashlib.md5(f\"{time.time()}{str(input_data)[:100]}\".encode()).hexdigest()[:12]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    metadata = metadata or {}\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Processing input for conversation {conversation_id}\")\n",
    "        \n",
    "        # Step 1: Enhanced Classification\n",
    "        classification = classify_agent(input_data)\n",
    "        fmt = classification.format\n",
    "        intent = classification.intent\n",
    "        \n",
    "        logger.info(f\"Classified as {fmt.value} with intent {intent.value} (confidence: {classification.confidence})\")\n",
    "        \n",
    "        # Step 2: Route to appropriate agent with enhanced error handling\n",
    "        result = None\n",
    "        \n",
    "        if fmt == InputFormat.JSON:\n",
    "            try:\n",
    "                if isinstance(input_data, str):\n",
    "                    payload = json.loads(input_data)\n",
    "                elif isinstance(input_data, dict):\n",
    "                    payload = input_data\n",
    "                else:\n",
    "                    payload = {\"raw_data\": str(input_data)[:1000]}\n",
    "            except (TypeError, json.JSONDecodeError) as e:\n",
    "                logger.warning(f\"JSON parsing failed: {e}\")\n",
    "                payload = {\"raw_data\": str(input_data)[:1000], \"parse_error\": str(e)}\n",
    "            \n",
    "            result = json_agent(payload, conversation_id, intent)\n",
    "            \n",
    "        elif fmt == InputFormat.PDF:\n",
    "            if not isinstance(input_data, bytes):\n",
    "                try:\n",
    "                    input_data = input_data.encode('utf-8')\n",
    "                except Exception:\n",
    "                    input_data = str(input_data).encode('utf-8', errors='ignore')\n",
    "            \n",
    "            result = pdf_agent(input_data, conversation_id)\n",
    "            \n",
    "        else:  # Email or fallback\n",
    "            if isinstance(input_data, bytes):\n",
    "                try:\n",
    "                    content = input_data.decode('utf-8', errors='ignore')\n",
    "                except Exception:\n",
    "                    content = str(input_data)\n",
    "            else:\n",
    "                content = str(input_data)\n",
    "            \n",
    "            result = email_agent(content, conversation_id, intent)\n",
    "        \n",
    "        # Step 3: Log to shared memory\n",
    "        memory_entry = MemoryEntry(\n",
    "            conversation_id=conversation_id,\n",
    "            timestamp=datetime.now(timezone.utc).isoformat(),\n",
    "            source=\"user_input\",\n",
    "            data_type=fmt.value,\n",
    "            intent=intent.value,\n",
    "            extracted_values=result.data if result else {},\n",
    "            agent_used=result.agent_used if result else \"None\",\n",
    "            processing_result=asdict(result) if result else {}\n",
    "        )\n",
    "        \n",
    "        shared_memory.log(memory_entry)\n",
    "        \n",
    "        # Step 4: Compile comprehensive response\n",
    "        total_processing_time = time.time() - start_time\n",
    "        \n",
    "        response = {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"classification\": {\n",
    "                \"format\": fmt.value,\n",
    "                \"intent\": intent.value,\n",
    "                \"confidence\": classification.confidence,\n",
    "                \"reasoning\": classification.reasoning\n",
    "            },\n",
    "            \"processing_result\": asdict(result) if result else {},\n",
    "            \"context\": {\n",
    "                \"conversation_summary\": shared_memory.get_conversation_summary(conversation_id),\n",
    "                \"total_interactions\": len(shared_memory.get_context(conversation_id))\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"total_processing_time\": total_processing_time,\n",
    "                \"agent_processing_time\": result.processing_time if result else 0\n",
    "            },\n",
    "            \"metadata\": metadata,\n",
    "            \"status\": \"success\" if result and result.success else \"partial_failure\"\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Processing completed for {conversation_id} in {total_processing_time:.2f}s\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        total_processing_time = time.time() - start_time\n",
    "        logger.exception(f\"Processing failed for conversation {conversation_id}: {e}\")\n",
    "        \n",
    "        error_response = {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"error\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"status\": \"error\",\n",
    "            \"performance\": {\n",
    "                \"total_processing_time\": total_processing_time\n",
    "            },\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "        return error_response\n",
    "\n",
    "# ----------- UTILITY FUNCTIONS ------------\n",
    "def get_conversation_history(conversation_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get full conversation history with analytics\"\"\"\n",
    "    entries = shared_memory.get_context(conversation_id, limit=100)\n",
    "    summary = shared_memory.get_conversation_summary(conversation_id)\n",
    "    \n",
    "    return {\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"summary\": summary,\n",
    "        \"entries\": [asdict(entry) for entry in entries],\n",
    "        \"total_entries\": len(entries)\n",
    "    }\n",
    "\n",
    "def health_check() -> Dict[str, Any]:\n",
    "    \"\"\"System health check\"\"\"\n",
    "    try:\n",
    "        # Test Groq connection\n",
    "        test_response = groq_completion(\"Hello\", response_format=\"json\", max_retries=1)\n",
    "        groq_status = \"healthy\"\n",
    "    except Exception as e:\n",
    "        groq_status = f\"unhealthy: {str(e)}\"\n",
    "    \n",
    "    # Test database connection\n",
    "    try:\n",
    "        test_entry = MemoryEntry(\n",
    "            conversation_id=\"health_check\",\n",
    "            timestamp=datetime.now(timezone.utc).isoformat(),\n",
    "            source=\"system\",\n",
    "            data_type=\"test\",\n",
    "            intent=\"health_check\",\n",
    "            extracted_values={},\n",
    "            agent_used=\"system\",\n",
    "            processing_result={}\n",
    "        )\n",
    "        db_status = \"healthy\" if shared_memory.log(test_entry) else \"unhealthy\"\n",
    "    except Exception as e:\n",
    "        db_status = f\"unhealthy: {str(e)}\"\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\" if groq_status == \"healthy\" and db_status == \"healthy\" else \"degraded\",\n",
    "        \"components\": {\n",
    "            \"groq_api\": groq_status,\n",
    "            \"database\": db_status\n",
    "        },\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. PDF (bytes)\n",
    "    with open(\"sample.pdf\", \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "    pdf_result = process_input(pdf_bytes, conversation_id=\"test_pdf\")\n",
    "    print(\"=== PDF RESULT ===\")\n",
    "    print(json.dumps(pdf_result, indent=2))\n",
    "\n",
    "    # 2. JSON (dict or string)\n",
    "    with open(\"sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        json_payload = json.load(f)\n",
    "    # You can either pass the dict directly...\n",
    "    json_result = process_input(json_payload, conversation_id=\"test_json\")\n",
    "    # ...or read it as raw text:\n",
    "    # raw_json_str = json.dumps(json_payload)\n",
    "    # json_result = process_input(raw_json_str, conversation_id=\"test_json\")\n",
    "    print(\"\\n=== JSON RESULT ===\")\n",
    "    print(json.dumps(json_result, indent=2))\n",
    "\n",
    "    # 3. Plain text (bytes or str)\n",
    "    # If you have sample.txt as UTF-8:\n",
    "    with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text_content = f.read()\n",
    "    text_result = process_input(text_content, conversation_id=\"test_txt\")\n",
    "    print(\"\\n=== TEXT RESULT ===\")\n",
    "    print(json.dumps(text_result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
